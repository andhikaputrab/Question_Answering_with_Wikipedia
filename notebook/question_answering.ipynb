{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andhi\\anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "from haystack import Pipeline\n",
    "from haystack import Document\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.document_stores.mongodb_atlas import MongoDBAtlasDocumentStore\n",
    "from haystack_integrations.components.retrievers.mongodb_atlas import MongoDBAtlasEmbeddingRetriever\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_id = stopwords.words('indonesian')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Fungsi untuk ekstraksi kata kunci\n",
    "def extract_keywords(question):\n",
    "    count = CountVectorizer(ngram_range=(1, 2), stop_words=stop_words_id).fit([question])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([question])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    top_n = 5\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    return keywords\n",
    "\n",
    "# Fungsi untuk mengambil artikel dari Wikipedia berdasarkan kata kunci\n",
    "def fetch_wikipedia_articles(question):\n",
    "    wikipedia = wikipediaapi.Wikipedia(user_agent='final_project', language='id')\n",
    "    keywords = extract_keywords(question)\n",
    "    \n",
    "    documents = []\n",
    "    try:\n",
    "        for keyword in keywords:\n",
    "            page = wikipedia.page(keyword)\n",
    "            if page.exists():\n",
    "                documents.append({\"content\": page.text, \"meta\": {\"title\": keyword}})\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        documents.append({\"content\": e.options[0], \"meta\": {\"title\": keyword}})\n",
    "    except wikipedia.exceptions.HTTPTimeoutError:\n",
    "        documents.append([])\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "# def fetch_biology_articles(question):\n",
    "#     wikipedia = wikipediaapi.Wikipedia(user_agent='Final_Project', language='en')\n",
    "\n",
    "#     # Mengambil halaman kategori Biologi\n",
    "#     category_page = wikipedia.page(\"Category:Biology\")\n",
    "\n",
    "#     # Mendapatkan kata kunci dari pertanyaan\n",
    "#     keywords = extract_keywords(question)\n",
    "\n",
    "#     # Mengambil artikel dalam kategori Biologi yang relevan dengan kata kunci\n",
    "#     biology_articles = []\n",
    "#     for page in category_page.categorymembers.values():  # Menggunakan categorymembers, bukan categories\n",
    "#         if any(keyword.lower() in page.title.lower() for keyword in keywords):\n",
    "#             biology_articles.append({\n",
    "#                 \"content\": page.text,\n",
    "#                 \"meta\": {\"title\": page.title}\n",
    "#             })\n",
    "    \n",
    "#     return biology_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"apa itu DNA?\"\n",
    "\n",
    "# # Get wikipedia documents\n",
    "# wikipedia_documents = fetch_wikipedia_articles(question)\n",
    "# wikipedia_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env MONGO_CONNECTION_STRING=mongodb+srv://andhikaputrab:finalProjectQA@cluster0.yypyg.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb+srv://andhikaputrab:finalProjectQA@cluster0.yypyg.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv('MONGO_CONNECTION_STRING'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk koneksi ke MongoDB\n",
    "def mongodb_connection():\n",
    "    database_name = \"question_answering\"\n",
    "    collection_name = \"wikipedia_documents\"\n",
    "\n",
    "    document_store = MongoDBAtlasDocumentStore(\n",
    "        database_name=database_name, \n",
    "        collection_name=collection_name, \n",
    "        vector_search_index=\"vector_index\"\n",
    "    )\n",
    "    \n",
    "    return document_store\n",
    "\n",
    "document_store = mongodb_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline untuk menyimpan dokumen ke MongoDB\n",
    "def document_pipeline(document_store):\n",
    "    pipeline_storing_mongodb = Pipeline()\n",
    "\n",
    "    pipeline_storing_mongodb.add_component(\"cleaner\", DocumentCleaner())\n",
    "    pipeline_storing_mongodb.add_component(\"splitter\", DocumentSplitter(split_by=\"word\", split_length=256, split_overlap=100))\n",
    "    pipeline_storing_mongodb.add_component(\"embedder\", SentenceTransformersDocumentEmbedder())\n",
    "    pipeline_storing_mongodb.add_component(\"writer\", DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP))\n",
    "\n",
    "    pipeline_storing_mongodb.connect(\"cleaner\", \"splitter\")\n",
    "    pipeline_storing_mongodb.connect(\"splitter\", \"embedder\")\n",
    "    pipeline_storing_mongodb.connect(\"embedder\", \"writer\")\n",
    "\n",
    "    return pipeline_storing_mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in wikipedia_documents]\n",
    "\n",
    "# pipeline_storing_mongodb = preprocess_documents(document_store=document_store)\n",
    "\n",
    "# pipeline_storing_mongodb.run({\"documents\": documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline untuk menghasilkan jawaban dari pertanyaan\n",
    "def answer_generator_pipeline(document_store):\n",
    "    template = \"\"\"\n",
    "        given these documents, answer the question based on these documents in a sentence. Documents:\n",
    "        {% for document in documents %}\n",
    "            {{ document.content }}\n",
    "        {% endfor %}\n",
    "        Question: {{query}}\n",
    "    \"\"\"\n",
    "\n",
    "    generator = OllamaGenerator(\n",
    "        model=\"llama3.2\",\n",
    "        url=\"http://localhost:11434/api/generate\",\n",
    "        # generation_kwargs={\n",
    "        #     \"num_predict\": 100,\n",
    "        #     \"temperature\": 0.5\n",
    "        # }\n",
    "    )\n",
    "    \n",
    "    pipeline_generate_answers = Pipeline()\n",
    "    pipeline_generate_answers.add_component('embedder', SentenceTransformersTextEmbedder())\n",
    "    pipeline_generate_answers.add_component('retriever', MongoDBAtlasEmbeddingRetriever(document_store=document_store))\n",
    "    pipeline_generate_answers.add_component('builder', PromptBuilder(template=template))\n",
    "    pipeline_generate_answers.add_component('generator', generator)\n",
    "\n",
    "    pipeline_generate_answers.connect(\"embedder\", \"retriever\")\n",
    "    pipeline_generate_answers.connect(\"retriever\", \"builder\")\n",
    "    pipeline_generate_answers.connect(\"builder\", \"generator\")\n",
    "    \n",
    "    return pipeline_generate_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menyimpan dokumen ke dalam pipeline\n",
    "def save_documents(documents, document_store):\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents to store.\")\n",
    "    # Proceed to store documents\n",
    "    pipeline_storing_mongodb = document_pipeline(document_store=document_store)\n",
    "    pipeline_storing_mongodb.run({\"documents\": documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menjawab pertanyaan dengan menggunakan dokumen Wikipedia\n",
    "def ask_question(query, document_store):\n",
    "    # Ambil dokumen Wikipedia berdasarkan kata kunci yang diekstraksi dari pertanyaan\n",
    "    wikipedia_documents = fetch_wikipedia_articles(query)\n",
    "    \n",
    "    # Konversi dokumen menjadi format pada Haystack\n",
    "    document_objects = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in wikipedia_documents]\n",
    "    \n",
    "    # Simpan dokumen ke MongoDB\n",
    "    save_documents(document_objects, document_store)\n",
    "    \n",
    "    # Buat pipeline untuk menghasilkan jawaban\n",
    "    pipeline_generate_answers = answer_generator_pipeline(document_store)\n",
    "    \n",
    "    # Jalankan pipeline untuk mendapatkan jawaban\n",
    "    response = pipeline_generate_answers.run({\n",
    "        \"embedder\": {\"text\": query},\n",
    "        \"builder\": {\"query\": query}\n",
    "    })\n",
    "    \n",
    "    return response['generator']['replies'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andhi\\anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 2/2 [01:02<00:00, 31.45s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban: Photosynthesis adalah proses biologis oleh mana tumbuhan dan beberapa organisme lainnya mengubah sinar matahari, air, dan karbon dioksida menjadi energi kimia dalam bentuk glukosa dan oksigen. Proses ini memungkinkan tanaman untuk berkembang dan tumbuh serta menyediakan energi bagi organisme lain yang mengkonsumsinya.\n",
      "\n",
      "Dalam proses photosynthesis, sinar matahari digunakan sebagai sumber energi, sedangkan air diubah menjadi oksigen dan karbon dioksida. Glukosa hasil dari fotosintesis kemudian dapat digunakan oleh tanaman untuk pertumbuhan dan perkembangannya, sementara oksigen dihasilkan sebagai produk sampingan yang berfungsi sebagai udara yang diperlukan bagi kehidupan.\n",
      "\n",
      "Fotosintesis terdiri atas dua tahap utama: fotosintesis intensif (light-dependent reaction) dan fotosintesis spesifik (light-independent reaction). Fotosintesis intensif terjadi di chloroplast dan memerlukan sinar matahari sebagai sumber energi, sedangkan fotosintesis spesifik terjadi di miselium tanaman dan tidak memerlukan sinar matahari secara langsung.\n"
     ]
    }
   ],
   "source": [
    "query = \"Jelaskan apa yang dimaksud dengan photosynthesis\"  # Bahasa Indonesia\n",
    "answer_id = ask_question(query, document_store)\n",
    "print(\"Jawaban:\", answer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.evaluators import DocumentMRREvaluator\n",
    "MRR_evaluator = DocumentMRREvaluator()\n",
    "MRR_result = MRR_evaluator.run(\n",
    "    ground_truth_documents=[[s] for s in selected_contexts],\n",
    "    retrieved_documents=results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
